{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e9c9a",
   "metadata": {},
   "source": [
    "# üìì Brick E2 ‚Äì Cloud Object Storage & Cloud-Native Formats\n",
    "\n",
    "**Part of Modern GIS Bricks**  \n",
    "\n",
    "**Objective:** In this notebook, you will learn how to create, manage, and query an Apache Iceberg table using **PyIceberg**, then read it with PyIceberg and **DuckDB**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4622cf",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Catalog Configuration\n",
    "Load a local Iceberg catalog pointed at your S3 warehouse (or local filesystem) and configure AWS credentials via environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e6bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "# 1. Make a local folder for both metadata + data\n",
    "warehouse_dir = os.path.abspath(\"iceberg_warehouse\")\n",
    "os.makedirs(warehouse_dir, exist_ok=True)\n",
    "\n",
    "# 2. Point the catalog at a local SQLite file\n",
    "sqlite_path = os.path.join(warehouse_dir, \"catalog.db\")\n",
    "db_uri = f\"sqlite:///{sqlite_path}\"\n",
    "warehouse_uri = f\"file://{warehouse_dir}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6732d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create / load the catalog\n",
    "catalog = load_catalog(\n",
    "    name=\"local_sql\",\n",
    "    type=\"sql\",\n",
    "    uri=db_uri,\n",
    "    warehouse=warehouse_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3766d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Ensure the namespace exists\n",
    "ns = (\"modern_gis\",)\n",
    "if ns not in catalog.list_namespaces():\n",
    "    catalog.create_namespace(ns)\n",
    "print(\"‚úÖ Local SQL catalog ready:\", db_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c900dc",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Create an Iceberg Table\n",
    "Define a schema matching the CSV columns plus a WKB geometry, then create the Iceberg table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.schema import Schema\n",
    "from pyiceberg.types import NestedField, DateType, StringType, LongType, BinaryType\n",
    "\n",
    "schema = Schema(\n",
    "    NestedField(1, \"sale_id\", StringType(), required=False),\n",
    "    NestedField(2, \"price\", LongType(),  required=False),\n",
    "    NestedField(3, \"sale_date\",  DateType(),  required=False),\n",
    "    NestedField(4, \"geom_wkb\",   BinaryType(),  required=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46490e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = (\"modern_gis\", \"kingco_sales\")\n",
    "table = catalog.create_table(\n",
    "    identifier=table_id,\n",
    "    schema=schema,\n",
    "    location=f\"{warehouse_uri}/kingco_sales\"\n",
    ")\n",
    "print(\"üÜï Created table:\", table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19d35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load the SQL (SQLite) catalog\n",
    "catalog = load_catalog(\n",
    "    name=\"local_sql\",       # any name you like\n",
    "    type=\"sql\",             # use the SQL Catalog implementation\n",
    "    uri=db_uri,             # where to write the SQLite DB\n",
    "    warehouse=warehouse_uri # where to store table data\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SQL Catalog ready\")\n",
    "print(\"Properties:\", catalog.properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade8cc2f",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Ingest Data by Year (2020‚ÄìPresent)\n",
    "Read the CSV, generate WKB geometry, and append data for each year to create distinct snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c60dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# read the original Parquet\n",
    "df = pd.read_parquet(\"data/kingco_sales.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c1ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a GeoDataFrame with a true geometry column\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df,\n",
    "    geometry = [Point(x, y) for x, y in zip(df.longitude, df.latitude)],\n",
    "    crs      = \"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the old lon/lat if you like\n",
    "gdf = gdf.drop(columns=[\"longitude\",\"latitude\", \"city\"])\n",
    "\n",
    "gdf[\"geom_wkb\"] = gdf.geometry.apply(lambda g: g.wkb)\n",
    "gdf = gdf.drop(columns=\"geometry\")\n",
    "\n",
    "# write GeoParquet\n",
    "gdf.to_parquet(\"data/kingco_sales_geoparquet.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the full Parquet into a PyArrow Table and convert to pandas\n",
    "arrow_tbl = pq.read_table('data/kingco_sales_geoparquet.parquet')\n",
    "df = arrow_tbl.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a437b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure sale_date is a datetime and extract the year\n",
    "df[\"year\"] = pd.to_datetime(df[\"sale_date\"]).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each year, append that slice, and create a new snapshot\n",
    "for year in sorted(df[\"year\"].unique()):\n",
    "    df_year = df[df[\"year\"] == year].copy().drop(columns=[\"year\"])\n",
    "    \n",
    "    # Reorder to match your schema exactly\n",
    "    df_year = df_year[[\"sale_id\", \"price\", \"sale_date\", \"geom_wkb\"]]\n",
    "    \n",
    "    # 6) Build an Arrow table (now with timestamp[us])\n",
    "    arrow_year = pa.Table.from_pandas(df_year, preserve_index=False)\n",
    "    \n",
    "    # 7) Append and commit a snapshot for this year\n",
    "    table.append(arrow_year)\n",
    "    snap_id = table.current_snapshot().snapshot_id\n",
    "    print(f\"‚úÖ Appended year {year} ‚Üí snapshot {snap_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e3602",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Snapshot History & Time Travel\n",
    "List all snapshots (one per ingested year) and demonstrate loading a mid-range year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_snapshots(tbl):\n",
    "    print(\"Snapshots (yearly commits):\")\n",
    "    for snap in tbl.snapshots():    # ‚Üê note the () here\n",
    "        ts = snap.timestamp_ms / 1000\n",
    "        from datetime import datetime\n",
    "        print(f\"- ID: {snap.snapshot_id}, Time: {datetime.fromtimestamp(ts)}\")\n",
    "\n",
    "show_snapshots(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56374829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Grab all snapshots\n",
    "snaps = table.snapshots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c596180",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Snapshot history and row counts:\")\n",
    "for snap in snaps:\n",
    "    # read that snapshot into a pandas DataFrame\n",
    "    df_snap = table.scan(snapshot_id=snap.snapshot_id).to_pandas()\n",
    "    print(f\"- Snapshot {snap.snapshot_id} @ {ts}: {len(df_snap)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8359202c",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Schema Evolution\n",
    "Add a new optional `zipcode` column to the schema if present in CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ff82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyiceberg.types import NestedField, StringType\n",
    "\n",
    "# 1Ô∏è‚É£ Load your existing Iceberg table\n",
    "table = catalog.load_table((\"modern_gis\", \"kingco_sales\"))\n",
    "\n",
    "schema = table.schema()\n",
    "\n",
    "# 2Ô∏è‚É£ Compute the next field ID\n",
    "existing_ids = [col.field_id for col in schema.columns]\n",
    "next_id = max(existing_ids) + 1\n",
    "\n",
    "# 2Ô∏è‚É£ Pick the next available field ID\n",
    "existing_ids = [col.field_id for col in schema.columns]\n",
    "next_id      = max(existing_ids) + 1\n",
    "\n",
    "# 3Ô∏è‚É£ Evolve the schema, adding \"city\" as an optional string\n",
    "with table.update_schema() as upd:\n",
    "    upd.add_column(\n",
    "        [\"city\"],               # path to the new field\n",
    "        StringType(),           # its Iceberg type\n",
    "        required=False,        # optional column\n",
    "    )\n",
    "    \n",
    "# 3Ô∏è‚É£ Verify the new schema includes \"city\"\n",
    "print(\"‚úÖ Schema after evolution:\")\n",
    "print(table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4791ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "# 2Ô∏è‚É£ Build a small DataFrame with the new data, including `city`\n",
    "df_new = pd.read_parquet(\"data/kingco_sales_geoparquet.parquet\")\n",
    "\n",
    "# 3Ô∏è‚É£ Convert to a PyArrow Table (columns auto-inferred)\n",
    "arrow_new = pa.Table.from_pandas(df_new, preserve_index=False)\n",
    "\n",
    "# 4Ô∏è‚É£ Append via PyIceberg\n",
    "table.append(arrow_new)\n",
    "\n",
    "# 5Ô∏è‚É£ Check the new snapshot\n",
    "new_snap = table.current_snapshot().snapshot_id\n",
    "print(f\"‚úÖ Appended new rows with city ‚Üí snapshot {new_snap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ccba4e",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Query via DuckDB\n",
    "Use DuckDB‚Äôs Iceberg extension to query the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a8484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "# 4Ô∏è‚É£ Spin up DuckDB and register that DataFrame as a table\n",
    "con = duckdb.connect()\n",
    "con.register(\"sales\", df)\n",
    "\n",
    "# Run whatever SQL you like. Examples:\n",
    "\n",
    "# All rows sold in 2021\n",
    "df_2021 = con.execute(\"\"\"\n",
    "  SELECT * \n",
    "  FROM sales\n",
    "  WHERE EXTRACT(YEAR FROM sale_date) = 2021;\n",
    "\"\"\").df()\n",
    "print(f\"Found {len(df_2021)} rows in 2021\")\n",
    "\n",
    "# 5b) Year‚Äêover‚Äêyear row counts by snapshot (if you still want that)\n",
    "# Note: you could also loop over table.snapshots() and re‚Äêscan, but here's \n",
    "# just a SQL that groups by the year-string.\n",
    "df_counts = con.execute(\"\"\"\n",
    "  SELECT EXTRACT(YEAR FROM sale_date) AS year, COUNT(*) as cnt\n",
    "    FROM sales\n",
    "   GROUP BY EXTRACT(YEAR FROM sale_date)\n",
    "   ORDER BY EXTRACT(YEAR FROM sale_date)\n",
    "\"\"\").df()\n",
    "print(df_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8d7d7",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Badge Proof\n",
    "Write a JSON summary of table state for automated badge verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2faf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 1Ô∏è‚É£ (Re)load your Iceberg table\n",
    "table = catalog.load_table((\"modern_gis\", \"kingco_sales\"))\n",
    "\n",
    "# 2Ô∏è‚É£ Unpack the table name\n",
    "namespace, tablename = table.name()   # returns the Identifier tuple\n",
    "\n",
    "# 3Ô∏è‚É£ Fetch snapshots and schema\n",
    "snaps  = table.snapshots()            # call the method to get the list\n",
    "schema = table.schema()               # call the method to get the Schema\n",
    "\n",
    "# 4Ô∏è‚É£ Build the proof dict\n",
    "proof = {\n",
    "    \"table\":     f\"{namespace}.{tablename}\",\n",
    "    \"snapshots\": [s.snapshot_id for s in snaps],\n",
    "    \"columns\":   [col.name for col in schema.columns]\n",
    "}\n",
    "\n",
    "# 5Ô∏è‚É£ Write to JSON\n",
    "with open(\"badge_proof.json\", \"w\") as f:\n",
    "    json.dump(proof, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Badge proof written to badge_proof.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba1418",
   "metadata": {},
   "source": [
    "# Create Iceberg tables on AWS S3 with AWS Glue (Optional)\n",
    "Use this code to optionally create tables on AWS using S3 and Glue, a catalog service similar to the SQLite database we used in the local version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11132b2",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Create Glue Tables\n",
    "Create and clean tables on AWS S3 and Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in AWS keys and environment variables\n",
    "AWS_KEY    = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "AWS_SECRET = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "AWS_REGION = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "BUCKET     = os.environ[\"YOUR_BUCKET\"]  # e.g. \"my-iceberg-bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a947bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish your Glue catalog\n",
    "\n",
    "WAREHOUSE  = f\"s3://{BUCKET}/iceberg_warehouse\"\n",
    "\n",
    "glue_cat = load_catalog(\n",
    "    name=\"glue_cat\",\n",
    "    type=\"glue\",\n",
    "    warehouse=WAREHOUSE,\n",
    "    **{\n",
    "        \"client.access-key-id\":     AWS_KEY,\n",
    "        \"client.secret-access-key\": AWS_SECRET,\n",
    "        \"client.region\":            AWS_REGION,\n",
    "    }\n",
    ")\n",
    "\n",
    "# ensure the namespace exists\n",
    "ns = (\"modern_gis\",)\n",
    "if ns not in glue_cat.list_namespaces():\n",
    "    glue_cat.create_namespace(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69428e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tables\n",
    "\n",
    "schema = Schema(\n",
    "    NestedField(1, \"sale_id\", StringType(), required=False),\n",
    "    NestedField(2, \"price\", LongType(),  required=False),\n",
    "    NestedField(3, \"sale_date\",  DateType(),  required=False),\n",
    "    NestedField(4, \"geom_wkb\",   BinaryType(),  required=False),\n",
    ")\n",
    "\n",
    "table_id = (\"modern_gis\", \"sales_data\")\n",
    "try:\n",
    "    table = glue_cat.load_table(table_id)\n",
    "except Exception:\n",
    "    table = glue_cat.create_table(\n",
    "        identifier=table_id,\n",
    "        schema=schema,\n",
    "        location=f\"{WAREHOUSE}/modern_gis/sales_data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac5050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# 1Ô∏è‚É£ Read your GeoParquet into a PyArrow Table \n",
    "arrow_tbl = pq.read_table(\"data/kingco_sales_geoparquet.parquet\")\n",
    "\n",
    "# 2Ô∏è‚É£ Convert to pandas for easy geometry ‚Üí WKB transformation\n",
    "df = arrow_tbl.to_pandas().head()\n",
    "\n",
    "# 3Ô∏è‚É£ Select & cast exactly the columns in your Iceberg schema \n",
    "df = df[[\"sale_id\", \"price\", \"sale_date\", \"geom_wkb\"]]\n",
    "\n",
    "\n",
    "# 4Ô∏è‚É£ Convert back to PyArrow and append into your Iceberg table \n",
    "arrow_to_append = pa.Table.from_pandas(df, preserve_index=False)\n",
    "table.append(arrow_to_append)\n",
    "\n",
    "print(\"‚úÖ Appended GeoParquet data ‚Üí snapshot\", table.current_snapshot().snapshot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume you already have your GlueCatalog or SQLCatalog in `catalog`\n",
    "# and your table identifier:\n",
    "table_id = (\"modern_gis\", \"sales_data\")\n",
    "\n",
    "# Drop just the metadata entry\n",
    "glue_cat.drop_table(table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e628f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and delete tables\n",
    "\n",
    "import boto3, os\n",
    "\n",
    "bucket = os.environ[\"YOUR_BUCKET\"]\n",
    "prefix = \"iceberg_warehouse/modern_gis/sales_data/\"\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "objs = s3.Bucket(bucket).objects.filter(Prefix=prefix)\n",
    "deleted = objs.delete()\n",
    "\n",
    "print(f\"Deleted {len(deleted)} S3 objects under {prefix}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
